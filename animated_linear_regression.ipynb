{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries.\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import functools\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# The %matplotlib magic function along with notebook enables some interactive features.\n",
    "%matplotlib notebook\n",
    "#%matplotlib widget\n",
    "\n",
    "\"\"\"\n",
    "Generate data points for plotting the error surface.\n",
    "\"\"\"\n",
    "def calculateErrorSurface(y, X):\n",
    "    # Generate values for parameter space.\n",
    "    N = 200\n",
    "    a0 = np.linspace(-10.0, 14.0, N)\n",
    "    a1 = np.linspace(-10.0, 14.0, N)\n",
    "\n",
    "    A0, A1 = np.meshgrid(a0, a1)\n",
    "\n",
    "    # Generate points for plotting the cost-function surface.\n",
    "    J = np.zeros((N,N))\n",
    "    for iter1 in range(0, N):\n",
    "        for iter2 in range(0, N):\n",
    "            yhat = A0[iter1][iter2] + A1[iter1][iter2]*X\n",
    "            J[iter1][iter2] = (1/M)*np.sum( np.square(y - yhat)  )\n",
    "            \n",
    "    return J, A0, A1\n",
    "            \n",
    "\"\"\"\n",
    "Calculate closed-form solution using the normal equation.\n",
    "\"\"\"\n",
    "def calculateClosedFormSolution(X):\n",
    "    # Closed-form solution.\n",
    "    a_opt = np.linalg.pinv(np.transpose(X).dot(X)).dot(np.transpose(X).dot(y))\n",
    "    yhat = a_opt[0, 0] + a_opt[1, 0]*X\n",
    "    Joptimum = (1/M)*np.sum(np.power((y - yhat), 2) )\n",
    "    \n",
    "    return Joptimum, a_opt\n",
    "\n",
    "\"\"\"\n",
    "Batch gradient descent solution.\n",
    "\"\"\"\n",
    "def batchGradientDescent(alpha, n_iterations, X_b, y):\n",
    "    # Random initialization of parameters.\n",
    "    a = np.zeros((2,1))\n",
    "    a[0] = -10;\n",
    "    a[1] = -10;\n",
    "\n",
    "    # Create vector for parameter history.\n",
    "    a_hist = np.zeros((2, n_iterations+1))\n",
    "    # Initialize history vector.\n",
    "    a_hist[0, 0] = a[0]\n",
    "    a_hist[1, 0] = a[1]\n",
    "\n",
    "    # Batch gradient-descent loop.\n",
    "    for iteration in range(n_iterations):\n",
    "        gradients = -2/M * X_b.T.dot(y - X_b.dot(a))\n",
    "        a = a - alpha * gradients\n",
    "        a_hist[0, iteration+1] = a[0]\n",
    "        a_hist[1, iteration+1] = a[1]\n",
    "        \n",
    "    return a, a_hist\n",
    "\n",
    "\"\"\"\n",
    "Hypothesis Function\n",
    "\"\"\"\n",
    "def h(X_b, a):\n",
    "    return a.T.dot(X_b.T)\n",
    "    \n",
    "## --------------------------------------------------------   \n",
    "    \n",
    "# Number of examples.\n",
    "M = 1000\n",
    "\n",
    "X = 2 * np.random.rand(M, 1)\n",
    "y = 4 + 3 * X + np.random.randn(M, 1)\n",
    "\n",
    "# add x0 = 1 to each instance.\n",
    "X_b = np.c_[np.ones((M, 1)), X]\n",
    "    \n",
    "# Batch gradient descent solution.\n",
    "alpha = 0.1  # learning rate\n",
    "n_iterations = 1000\n",
    "\n",
    "# Calculate data point for plotting error surface.\n",
    "J, A0, A1 = calculateErrorSurface(y, X)\n",
    "\n",
    "# Calculate closed-form solution.\n",
    "Joptimum, a_opt = calculateClosedFormSolution(X_b)\n",
    "\n",
    "# Run batch gradient-descent algorithm.\n",
    "a, a_hist, a_history = batchGradientDescent(alpha, n_iterations, X_b, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.scatter(X, y)\n",
    "xl = []\n",
    "yl = []\n",
    "ln, = plt.plot(xl, yl, 'r--', animated=True, label='h(x)')\n",
    "plt.title('Gradiente Descendente')\n",
    "\n",
    "def init():\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    plt.legend(handles=[ln])\n",
    "    return ln,\n",
    "\n",
    "def update(frame):\n",
    "    theta = np.zeros((2,1))\n",
    "    theta[0] = a_hist[0,frame]\n",
    "    theta[1] = a_hist[1,frame]\n",
    "    f = functools.partial(h, a=theta)\n",
    "    y = f(X_b)\n",
    "    ln.set_data(X, y)\n",
    "    return ln,\n",
    "\n",
    "#ani = FuncAnimation(fig, update, frames=200, init_func=init, blit=False, interval=100)\n",
    "#display(HTML(ani.to_html5_video()))\n",
    "\n",
    "#ani.save('animated_linear_regression.mp4', fps=30, extra_args=['-vcodec', 'libx264'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cost-function surface.\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(A0, A1, J, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_xlabel('$a_0$', fontsize=14)\n",
    "ax.set_ylabel('$a_1$', fontsize=14)\n",
    "ax.set_zlabel('$J_e$', fontsize=14);\n",
    "plt.title('Cost-function\\'s Surface')\n",
    "ax.view_init(20, 45)\n",
    "#Show the plot.\n",
    "plt.show()\n",
    "# Save figure into file.\n",
    "plt.savefig(\"error_surface_bgd.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figure.        \n",
    "fig = plt.figure(figsize=(10,10))\n",
    "cp = plt.contour(A0, A1, J)\n",
    "plt.clabel(cp, inline=1, fontsize=10)\n",
    "plt.xlabel('$a_0$', fontsize=14)\n",
    "plt.ylabel('$a_1$', fontsize=14)\n",
    "plt.title('Cost-function\\'s Contour')\n",
    "plt.plot(a_opt[0], a_opt[1], c='r', marker='*')\n",
    "plt.plot(a_hist[0, :], a_hist[1, :], 'kx')\n",
    "plt.xticks(np.arange(-10, 12, step=2.0))\n",
    "plt.yticks(np.arange(-10, 12, step=2.0))\n",
    "plt.show()\n",
    "# save figure into file.\n",
    "plt.savefig(\"error_contour_bgd.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig, ax = plt.subplots()\n",
    "cp = plt.contour(A0, A1, J)\n",
    "plt.clabel(cp, inline=1, fontsize=10)\n",
    "plt.plot(a_opt[0], a_opt[1], c='r', marker='*')\n",
    "xl = []\n",
    "yl = []\n",
    "ln, = plt.plot(xl, yl, 'kx', animated=True)\n",
    "plt.title('Gradiente Descendente')\n",
    "plt.xticks(np.arange(-10, 12, step=2.0))\n",
    "plt.yticks(np.arange(-10, 12, step=2.0))\n",
    "\n",
    "def init():\n",
    "    ax.set_xlabel('$a_0$', fontsize=14)\n",
    "    ax.set_ylabel('$a_1$', fontsize=14)\n",
    "    return ln,\n",
    "\n",
    "def update(frame):\n",
    "    ln.set_data(a_hist[0, frame], a_hist[1, frame])\n",
    "    return ln,\n",
    "\n",
    "ani2 = FuncAnimation(fig, update, frames=200, init_func=init, blit=False, interval=1000)\n",
    "display(HTML(ani2.to_html5_video()))\n",
    "\n",
    "ani2.save('animated_linear_regression_contour.mp4', fps=30, extra_args=['-vcodec', 'libx264'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
